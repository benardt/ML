{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "oring_optim.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOEBxp50Tc5LFpAa2ahKwjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benardt/ML/blob/main/oring_optim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggsyQv9J2Ssp",
        "outputId": "ed984b20-9cfb-476e-a3f8-ca33adec7f90"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import shutil\n",
        "shutil.unpack_archive(\"/content/drive/My Drive/data/test.zip\", \"/tmp\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5HWR2Qzt9u8",
        "outputId": "5ef1cc84-697e-4daa-9967-51db829a15a5"
      },
      "source": [
        "!pip install thop 1>/dev/null\n",
        "!pip install optuna 1>/dev/null\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
        "import numpy as np\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "import thop\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evfpvq00Xgic",
        "outputId": "1cf30a90-6e49-4d26-90ad-c3a67366b5ef"
      },
      "source": [
        "!pip install livelossplot --quiet\n",
        "from livelossplot import PlotLosses\n",
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "from importlib import reload # reload "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=436dd6ef2e6039cfe4b6b2b3883c702682ceffaa7e183cfa30238af13f889027\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtCEn-TUFbyx",
        "outputId": "85fd2bd5-3004-4c6c-a743-53795e83a5d7"
      },
      "source": [
        "!rm -r './mylib.ipynb'\n",
        "!cp '/content/drive/My Drive/Colab Notebooks/mylib.ipynb' .\n",
        "import mylib as ml\n",
        "reload(ml)\n",
        "ml.mytest('This is a test.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './mylib.ipynb': No such file or directory\n",
            "importing Jupyter notebook from mylib.ipynb\n",
            "importing Jupyter notebook from mylib.ipynb\n",
            "test import... This is a test.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhe5ktgbxgNw"
      },
      "source": [
        "FOLDER = 'oring11'\n",
        "\n",
        "OUTPUT_SIZE = 63\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "\n",
        "shutil.unpack_archive('/content/drive/My Drive/data/'+FOLDER+'.zip', '/tmp')\n",
        "\n",
        "def get_data(isprint=False):\n",
        "\n",
        "    trans_in = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                          transforms.ToTensor()])\n",
        "    \n",
        "    interpol = transforms.InterpolationMode.NEAREST\n",
        "    trans_out = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                          transforms.Resize(OUTPUT_SIZE,interpolation=interpol),\n",
        "                          transforms.ToTensor()])   \n",
        "\n",
        "    class OringLandmarksDataset(Dataset):\n",
        "        \"\"\"Landmarks dataset.\"\"\"\n",
        "\n",
        "        def __init__(self, root_dir):\n",
        "            \"\"\"\n",
        "            Args:\n",
        "                root_dir (string): Directory with all the images.\n",
        "            \"\"\"\n",
        "\n",
        "            self.root_dir = root_dir\n",
        "            self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"PNGImages\"))))\n",
        "            self.masks = list(sorted(os.listdir(os.path.join(root_dir, \"Masks\"))))\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.imgs)\n",
        "\n",
        "        def transform(self, x, y):\n",
        "\n",
        "            image = trans_in(x)\n",
        "            mask = trans_out(y)\n",
        "            return image, mask\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            if torch.is_tensor(idx):\n",
        "                idx = idx.tolist()\n",
        "\n",
        "            img_name = os.path.join(self.root_dir, \"PNGImages\", self.imgs[idx])\n",
        "            im_invert = Image.open(img_name).convert('L')\n",
        "            image = ImageOps.invert(im_invert)\n",
        "\n",
        "            path = self.imgs[idx]\n",
        "\n",
        "            mask_name = os.path.join(self.root_dir, \"Masks\", self.masks[idx])\n",
        "            mask_invert = Image.open(mask_name).convert('L')\n",
        "            mask = ImageOps.invert(mask_invert)\n",
        "\n",
        "            x, y = self.transform(image, mask)\n",
        "            return x, y\n",
        "\n",
        "    trans_dataset = OringLandmarksDataset(root_dir='/tmp/'+FOLDER)\n",
        "\n",
        "    train_len = int(0.6*len(trans_dataset))\n",
        "    valid_len = len(trans_dataset)-train_len\n",
        "    TrainData, ValidData = torch.utils.data.random_split(trans_dataset,[train_len, valid_len])\n",
        "\n",
        "    mydataloader = { 'train':[],'valid':[]}\n",
        "    mydataloader['train'] = DataLoader(TrainData, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, pin_memory=True,num_workers=2)\n",
        "    mydataloader['valid'] = DataLoader(ValidData, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, pin_memory=True,num_workers=2)\n",
        "\n",
        "    if isprint:\n",
        "      for i_batch, (x,y) in enumerate(mydataloader['train']):\n",
        "        print(i_batch, x[0].size(), y[0].size())\n",
        "        x = transforms.functional.resize(x, OUTPUT_SIZE)\n",
        "        ml.images_show(x[0],y[0],5)\n",
        "        if i_batch == 2:\n",
        "          break\n",
        "\n",
        "    return mydataloader\n",
        "\n",
        "dataloader = get_data(isprint=False)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53_bpJYax4f5"
      },
      "source": [
        "def define_model(trial):\n",
        "\n",
        "  layers = []\n",
        "  out_features = [None,None]\n",
        "  kernels_size = [None,None]\n",
        "\n",
        "  i = 1 # first layer\n",
        "  out_features[i] = trial.suggest_int(\"n_channels{}\".format(i), 32, 128, 32)\n",
        "  kernels_size[i] = trial.suggest_int(\"ker_sizes{}\".format(i), 7, 23, 4)\n",
        "\n",
        "  # 1st layer\n",
        "  layers.append(nn.Conv2d(1,out_features[1],kernels_size[1],1,int((kernels_size[1]-1)/2)))\n",
        "  layers.append(nn.MaxPool2d(15,2,7))\n",
        "\n",
        "  # 2nd layer\n",
        "  layers.append(nn.Conv2d(out_features[1],128,5,1,2))\n",
        "  layers.append(nn.MaxPool2d(5,2,2))\n",
        "\n",
        "  # 3rd layer\n",
        "  layers.append(nn.Conv2d(128,128,3,1,1))\n",
        "  layers.append(nn.MaxPool2d(3,2,1))\n",
        "\n",
        "  # last layer\n",
        "  layers.append(nn.Conv2d(128,1,1,1,0))\n",
        "\n",
        "  return nn.Sequential(*layers)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqY2FtyUUSas"
      },
      "source": [
        "def objective(trial):\n",
        "    accuracy = ml.DiceLoss()\n",
        "    liveloss = PlotLosses()\n",
        "    model = define_model(trial).to(device)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
        "\n",
        "    #past_date = datetime.datetime.now().replace(microsecond=0)\n",
        "\n",
        "    all_logs = []\n",
        "    for epoch in range(EPOCHS):\n",
        "        logs = {}\n",
        "        for phase in ['train', 'valid']:\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for x, y in iter(dataloader[phase]):\n",
        "                x,y = x.to(device),y.to(device)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                    yhat = model(x)\n",
        "                    loss = criterion(yhat, y)\n",
        "                    for param in model.parameters():\n",
        "                        param.grad = None\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                else:\n",
        "                    model.eval()\n",
        "                    with torch.no_grad():\n",
        "                        yhat = model(x)\n",
        "                        loss = criterion(yhat, y)\n",
        "\n",
        "                # loss\n",
        "                running_loss += loss.detach() * x.detach().size(0)\n",
        "                # accuracy\n",
        "                preds = (yhat.detach() > 0.0)*1\n",
        "                acc = 100*(1 - accuracy(preds, y.detach()))\n",
        "                running_corrects += acc * x.detach().size(0)\n",
        "\n",
        "\n",
        "            epoch_loss = running_loss.item() / len(dataloader[phase].dataset)\n",
        "            epoch_acc = running_corrects.item() / len(dataloader[phase].dataset)\n",
        "\n",
        "            prefix = ''\n",
        "            if phase == 'valid':\n",
        "                prefix = 'val_'\n",
        "\n",
        "            logs[prefix + 'log loss'] = epoch_loss\n",
        "            logs[prefix + 'accuracy'] = epoch_acc\n",
        "\n",
        "        all_logs.append(logs)\n",
        "        #liveloss.update(logs)\n",
        "        #liveloss.send()\n",
        "\n",
        "        trial.report(epoch_acc, epoch)\n",
        "        # Handle pruning based on the intermediate value.\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    # future_date = datetime.datetime.now().replace(microsecond=0)\n",
        "    # difference = (future_date - past_date)\n",
        "    # total_seconds = int(difference.total_seconds())\n",
        "\n",
        "    flops, _params = thop.profile(model, inputs=(torch.randn(1, 1, 500, 500).to(device),), verbose=False)\n",
        "    return epoch_acc\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4Htt7MHvhuz",
        "outputId": "b0d2f2e9-476b-4829-e0d0-109e548d3aa6"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=20)\n",
        "\n",
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-07-07 20:26:06,527]\u001b[0m A new study created in memory with name: no-name-291aeb8b-1dca-4109-a802-9aadf56917e2\u001b[0m\n",
            "\u001b[32m[I 2021-07-07 20:29:26,581]\u001b[0m Trial 0 finished with value: 48.687142457916885 and parameters: {'n_channels1': 64, 'ker_sizes1': 7}. Best is trial 0 with value: 48.687142457916885.\u001b[0m\n",
            "\u001b[32m[I 2021-07-07 20:34:01,459]\u001b[0m Trial 1 finished with value: 74.84446508416623 and parameters: {'n_channels1': 96, 'ker_sizes1': 7}. Best is trial 1 with value: 74.84446508416623.\u001b[0m\n",
            "\u001b[32m[I 2021-07-07 20:36:06,904]\u001b[0m Trial 2 finished with value: 73.63445226196738 and parameters: {'n_channels1': 32, 'ker_sizes1': 7}. Best is trial 1 with value: 74.84446508416623.\u001b[0m\n",
            "\u001b[32m[I 2021-07-07 20:41:03,764]\u001b[0m Trial 3 finished with value: 90.8334840215676 and parameters: {'n_channels1': 64, 'ker_sizes1': 23}. Best is trial 3 with value: 90.8334840215676.\u001b[0m\n",
            "\u001b[32m[I 2021-07-07 20:44:23,871]\u001b[0m Trial 4 finished with value: 74.95924842188322 and parameters: {'n_channels1': 64, 'ker_sizes1': 7}. Best is trial 3 with value: 90.8334840215676.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Study statistics: \n",
            "  Number of finished trials:  5\n",
            "  Number of pruned trials:  0\n",
            "  Number of complete trials:  5\n",
            "Best trial:\n",
            "  Value:  90.8334840215676\n",
            "  Params: \n",
            "    n_channels1: 64\n",
            "    ker_sizes1: 23\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}